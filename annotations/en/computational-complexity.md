# Computational complexity theory

Computational complexity theory studies "how many resources are needed to solve a computational problem." Typical resources include time and space. It ranks problems and algorithms by resource growth rate, and studies lower bounds and unreachability.

## Core concepts
- Problems and algorithms: decision/search problems, algorithms and circuit models, Turing machine abstraction
- Resource metrics: time complexity, space complexity, randomization and parallel resources
- Complexity classes: P, NP, coNP, PH, PSPACE, EXP, etc.
- Difficulty and completeness: NP-complete, PSPACE-complete, reduction as a comparison tool
- Lower bounds and obstacles: relativization, natural proofs and other meta-results that hinder strong lower bound proofs

## Relation to this book
- Model selection: Traditional results mostly rely on models with complex structures or proof techniques; this book emphasizes understanding the universality of complex calculations starting from "minimalist programs".
- New clues: Simple rules demonstrate near-universal computing power and highly complex behavior, providing empirical material and intuition for understanding complexity boundaries and measurements.
- Distinction point: Computational irreducibility is the property of "prediction is irreducible"; complexity theory is based on resource overhead grading. The two are related but not identical.

## Further reading
- Wikipedia: https://zh.wikipedia.org/wiki/Computational Complexity Theory
- Textbooks: Sipser "Introduction to Computational Theory"; Arora & Barak "Computational Complexity: Modern Approaches"